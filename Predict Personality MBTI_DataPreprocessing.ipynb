{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sought-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "classical-electronics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\madhu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\dtypes\\inference.py:178: FutureWarning: Possible set union at position 2\n",
      "  re.compile(obj)\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "df_data = pd.read_csv('C:\\Sujata\\CompScience\\Winter 2021\\Data Mining\\Project Final\\mbti_1\\mbti_1.csv')\n",
    "\n",
    "# Removed |||\n",
    "df_data.posts = df_data.posts.replace(r'[|||]', r' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "immune-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diving the 16 MBTI type to only 4\n",
    "df_data['is_E'] = df_data['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\n",
    "df_data['is_S'] = df_data['type'].apply(lambda x: 1 if x[1] == 'S' else 0)\n",
    "df_data['is_T'] = df_data['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\n",
    "df_data['is_J'] = df_data['type'].apply(lambda x: 1 if x[3] == 'J' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serial-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the type column\n",
    "df_data = df_data.drop(['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rising-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the data according to I/E , N/S, F/T AND P/J. All of them has the post column\n",
    "df_data_IE = df_data.drop(['is_S', 'is_T', 'is_J'], axis=1)\n",
    "df_data_NS = df_data.drop(['is_E', 'is_T', 'is_J'], axis=1)\n",
    "df_data_FT = df_data.drop(['is_S', 'is_E', 'is_J'], axis=1)\n",
    "df_data_PJ = df_data.drop(['is_S', 'is_T', 'is_E'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlled-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating into training, validation and Testing data 60, 20, 20 - 5205, 1735 and 1735\n",
    "train_IE, validate_IE, test_IE = np.split(df_data_IE.sample(frac=1, random_state=42), [int(.6*len(df_data_IE)), int(.8*len(df_data_IE))])\n",
    "train_NS, validate_NS, test_NS = np.split(df_data_NS.sample(frac=1, random_state=42), [int(.6*len(df_data_NS)), int(.8*len(df_data_NS))])\n",
    "train_FT, validate_FT, test_FT = np.split(df_data_FT.sample(frac=1, random_state=42), [int(.6*len(df_data_FT)), int(.8*len(df_data_FT))])\n",
    "train_PJ, validate_PJ, test_PJ = np.split(df_data_PJ.sample(frac=1, random_state=42), [int(.6*len(df_data_PJ)), int(.8*len(df_data_PJ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stupid-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data to X and Y i.e. posts and labels\n",
    "#For IE labels\n",
    "X_train_IE = pd.DataFrame(train_IE['posts'])\n",
    "Y_train_IE = pd.DataFrame(train_IE['is_E'])\n",
    "X_validate_IE = pd.DataFrame(validate_IE['posts'])\n",
    "Y_validate_IE = pd.DataFrame(validate_IE['is_E'])\n",
    "X_test_IE = pd.DataFrame(test_IE['posts'])\n",
    "Y_test_IE = pd.DataFrame(test_IE['is_E'])\n",
    "\n",
    "#For IE labels\n",
    "X_train_NS = pd.DataFrame(train_NS['posts'])\n",
    "Y_train_NS = pd.DataFrame(train_NS['is_S'])\n",
    "X_validate_NS = pd.DataFrame(validate_NS['posts'])\n",
    "Y_validate_NS = pd.DataFrame(validate_NS['is_S'])\n",
    "X_test_NS = pd.DataFrame(test_NS['posts'])\n",
    "Y_test_NS = pd.DataFrame(test_NS['is_S'])\n",
    "\n",
    "#For IE labels\n",
    "X_train_FT = pd.DataFrame(train_FT['posts'])\n",
    "Y_train_FT = pd.DataFrame(train_FT['is_T'])\n",
    "X_validate_FT = pd.DataFrame(validate_FT['posts'])\n",
    "Y_validate_FT = pd.DataFrame(validate_FT['is_T'])\n",
    "X_test_FT = pd.DataFrame(test_FT['posts'])\n",
    "Y_test_FT = pd.DataFrame(test_FT['is_T'])\n",
    "\n",
    "#For IE labels\n",
    "X_train_PJ = pd.DataFrame(train_PJ['posts'])\n",
    "Y_train_PJ = pd.DataFrame(train_PJ['is_J'])\n",
    "X_validate_PJ = pd.DataFrame(validate_PJ['posts'])\n",
    "Y_validate_PJ = pd.DataFrame(validate_PJ['is_J'])\n",
    "X_test_PJ = pd.DataFrame(test_PJ['posts'])\n",
    "Y_test_PJ = pd.DataFrame(test_PJ['is_J'])\n",
    "\n",
    "#display(Y_train_IE.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hollow-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data Round 1\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation, remove words containing numbers, remove URL, remove @, '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "X_train_IE_clean = pd.DataFrame(X_train_IE.posts.apply(round1))\n",
    "X_validate_IE_clean = pd.DataFrame(X_validate_IE.posts.apply(round1))\n",
    "X_test_IE_clean = pd.DataFrame(X_test_IE.posts.apply(round1))\n",
    "\n",
    "X_train_NS_clean = pd.DataFrame(X_train_NS.posts.apply(round1))\n",
    "X_validate_NS_clean = pd.DataFrame(X_validate_NS.posts.apply(round1))\n",
    "X_test_NS_clean = pd.DataFrame(X_test_NS.posts.apply(round1))\n",
    "\n",
    "X_train_FT_clean = pd.DataFrame(X_train_FT.posts.apply(round1))\n",
    "X_validate_FT_clean = pd.DataFrame(X_validate_FT.posts.apply(round1))\n",
    "X_test_FT_clean = pd.DataFrame(X_test_FT.posts.apply(round1))\n",
    "\n",
    "X_train_PJ_clean = pd.DataFrame(X_train_PJ.posts.apply(round1))\n",
    "X_validate_PJ_clean = pd.DataFrame(X_validate_PJ.posts.apply(round1))\n",
    "X_test_PJ_clean = pd.DataFrame(X_test_PJ.posts.apply(round1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "widespread-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization - this is taken from SpaCy  \n",
    "\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "#stemmer = LancasterStemmer() # did not work\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re_tok.sub(r' \\1 ', s).split()\n",
    "    return tokens\n",
    "\n",
    "# Defining lemmatization\n",
    "wrl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amateur-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopword and also added the MBTI types to it\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['infp','infj','intp','intj','entp','enfp','istp','isfp','entj','istj','enfj','isfj','estp','esfp','esfj','estj'])\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    return [t for t in row if t not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gentle-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400001it [00:34, 11703.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#GloVe embedding loading\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings_index_glove = {}\n",
    "f = open('C:/Sujata/CompScience/Winter 2021/Data Mining/Project Final/glove.6B.300d.txt', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except:\n",
    "        continue\n",
    "    embeddings_index_glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "herbal-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s, embeddings_index):\n",
    "    words = str(s)\n",
    "    words = tokenize(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        w = wrl.lemmatize(w)\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cardiac-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create sentence vectors using the above function for training and validation set - this will convert the full sentence \n",
    "#into a vector \n",
    "\n",
    "xtrain_IE_glove = [sent2vec(x, embeddings_index_glove) for x in X_train_IE_clean['posts']]\n",
    "xvalid_IE_glove = [sent2vec(x, embeddings_index_glove) for x in X_validate_IE_clean['posts']]\n",
    " \n",
    "xtrain_FT_glove = [sent2vec(x, embeddings_index_glove) for x in X_train_FT_clean['posts']]\n",
    "xvalid_FT_glove = [sent2vec(x, embeddings_index_glove) for x in X_validate_FT_clean['posts']]\n",
    "\n",
    "xtrain_IE_glove = np.array(xtrain_IE_glove)\n",
    "xvalid_IE_glove = np.array(xvalid_IE_glove)\n",
    "\n",
    "xtrain_FT_glove = np.array(xtrain_FT_glove)\n",
    "xvalid_FT_glove = np.array(xvalid_FT_glove)\n",
    "\n",
    " \n",
    "# Generate Word vectors of test data is_E\n",
    "xtest_IE_glove = [sent2vec(x, embeddings_index_glove) for x in X_test_IE_clean['posts']]\n",
    "xtest_IE_glove = np.array(xtest_IE_glove)\n",
    "\n",
    "# Generate Word vectors of test data is_T\n",
    "xtest_FT_glove = [sent2vec(x, embeddings_index_glove) for x in X_test_FT_clean['posts']]\n",
    "xtest_FT_glove = np.array(xtest_FT_glove)\n",
    "\n",
    "#print(\"xtest_IE_glove.shape = \", xtest_IE_glove.shape)\n",
    "#xtrain_FT_glove[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
